# 爬虫

分类：
>通用爬虫
>聚焦爬虫
>增量爬虫

反爬机制：

>robots协议
User-Agent 请求载体的身份标识 
>


## requests模块

### 请求

#### get
```python
requests.get(url=url,params=param,headers=headers)

response = request.get(url)

response.text  #  文本数据
response.content # 二进制数据

获取二进制数据还可以用urllib.request.urlretrieve(url=url,filename='file_name'),但它不能进行UA伪装
```
#### post

```python
response = requests.post(url=url,data=data,headers=headers)
#获取响应数据:如果响应回来的数据为json，则可以直接调用响应对象的json方法获取json对象数据
json_data = response.json()
```


## 数据解析

### 正则

### bs4

#### 包安装：
```python
pip install bs4
pip install lxml
```
#### bs4使用流程：
导入模块 --> 实例化bs对象--->解析数据

#### 实例化对象

实例化时可以传入文件句柄，也可以是字符串类型。并指定解析引擎
```python
soup = BeautifulSoup(open('本地文件'), 'lxml')
soup = BeautifulSoup('字符串类型或者字节类型', 'lxml')
```
#### 数据解析

获取标签
`soup.a`
获取属性
` soup.a.attrs  获取a所有的属性和属性值，返回一个字典`
`soup.a.attrs['href']   获取href属性(soup.a['href']   也可简写为这种形式)`
获取内容
```py
soup.a.string
soup.a.text
soup.a.get_text()
```
find：找到第一个符合要求的标签
```py
 soup.find('a')  找到第一个符合要求的
 soup.find('a', title="xxx")
 soup.find('a', alt="xxx")
 soup.find('a', class_="xxx")
 soup.find('a', id="xxx")
```
find_all：找到所有符合要求的标签
```py
 soup.find_all('a')
 soup.find_all(['a','b']) 找到所有的a和b标签
 soup.find_all('a', limit=2)  限制前两个
```
根据选择器选择指定的内容

### xpath
