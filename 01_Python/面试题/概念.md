## 自由变量
```
1. 定义： 在编程语言理论中，自由变量是指在代码中使用的变量，但不是在当前作用域中定义的。
    
2. 闭包中的自由变量： 在闭包的上下文中，自由变量特指那些在外部函数中定义，但在内部函数中使用的变量。
    
3. 为什么称为"自由"：
    
    - 这些变量对于内部函数来说是"自由"的，因为它们不是内部函数的参数，也不是内部函数内部定义的局部变量。
    - 它们存在于一个更广泛的词法环境中，内部函数可以访问这个环境。
4. 与局部变量和全局变量的区别：
    
    - 它们不是内部函数的局部变量（因为不是在内部函数中定义的）。
    - 它们也不是全局变量（因为它们的作用域限于外部函数）。
5. 闭包中的重要性：
    
    - 这些自由变量是闭包实现的关键。
    - 闭包"捕获"了这些变量，使得即使外部函数执行完毕，内部函数仍能访问这些变量。

示例：
def outer(x):
    y = 10  # y 是自由变量
    def inner():
        return x + y  # x 和 y 都是自由变量
    return inner

closure = outer(5)
result = closure()  # 结果为 15

在这个例子中：

- `x` 和 `y` 都是自由变量，因为它们在 `inner` 函数中使用，但不是在 `inner` 函数中定义的。
- 这些变量对 `inner` 函数来说是"自由"的，因为它们存在于 `inner` 函数可以访问但不直接拥有的环境中
```


## 闭包

```
闭包（Closure）是指一个函数以及其相关的引用环境组合而成的实体。更具体地说：
1. 定义：
    - 闭包是由函数及其相关的引用环境组成的整体（实体）。
    - 它由内部函数和定义在外部函数中的变量构成。
2. 构成要素：
    - 外部函数
    - 内部函数
    - 外部函数中被内部函数引用的自由变量（非全局变量和非内部函数的局部变量）
3. 关键点：
    - 内部函数必须引用外部函数中的变量。
    - 外部函数必须返回内部函数。
4. 特性：
    - 闭包"记住"了创建它的环境。
    - 即使外部函数已经执行完毕，闭包仍然可以访问外部函数定义的变量。
5. 用途：
    - 实现数据隐藏和封装。
    - 创建函数工厂。
    - 实现装饰器。
```


## 装饰器

```
装饰器是一种设计模式，也是 Python 的一个特性。它是一个可调用的对象（通常是一个函数），用于修改或增强其他函数或类，而不需要直接修改被装饰的函数或类的源代码。

简单来说：
1. 装饰器是一个函数。
2. 这个函数接受一个函数作为输入。
3. 它返回一个新的函数。
4. 返回的新函数通常会在某种程度上修改或增强原始函数的行为。

装饰器的核心思想是 "包装" 另一个函数，以此来扩展其功能，同时不改变原函数的源代码
```

## 并行（Parallelism）：

```
1. 定义：真正同时执行多个任务。
    
2. 特点：
    
    - 需要多核处理器或多台机器。
    - 任务在物理上同时执行。
    - 适用于计算密集型任务。
3. 目标：提高整体处理速度和吞吐量。
    
4. 示例：多台计算机同时进行科学计算。
```
    

## 并发（Concurrency）：

```
1. 定义：在同一时间段内处理多个任务，但不一定同时执行。
    
2. 特点：
    
    - 可以在单核处理器上实现。
    - 通过任务切换来模拟同时执行。
    - 适用于 I/O 密集型任务。
3. 目标：提高程序的响应性和资源利用率。
    
4. 示例：一个网络服务器同时处理多个客户端请求。
```


## 迭代器

```
迭代器是一个实现了迭代器协议的对象。在 Python 中,迭代器协议包括两个方法:

- **iter**(): 返回迭代器对象本身
- **next**(): 返回容器中的下一个元素,如果没有更多元素则抛出 StopIteration 异常

2. 迭代器的特点:

- 惰性计算: 只有在需要时才会计算下一个元素,节省内存
- 单向遍历: 只能向前遍历,不能后退
- 状态保持: 记住当前遍历的位置
```

## 可迭代对象

```
可迭代对象是可以被迭代（遍历）的任何对象。它必须实现 iter() 方法或者 getitem() 方法。
```

## 生成器（Generator）

```
生成器是一种使用函数语法定义的迭代器。它使用 yield 语句而不是 return 来返回结果。
# 生成器函数
def countdown(n):
    while n > 0:
        yield n
        n -= 1

# 生成器表达式
squares = (x**2 for x in range(10))
```

## 简述websocket 协议及实现原理


websocket是给浏览器新建的一套（类似与http，基于Html5）协议，协议规定：（\r\n分割）浏览器和服务器连接之后不断开，以此完成：服务端向客户端主动推送消息。
全双工：可以同时双向发送数据

websocket协议额外做的一些操作
握手  ---->  连接线进行校验
加密  ----> payload_len=127/126/<=125   --> mask key 

传统socket是单相思，websocket是两情相悦
##本质
创建一个连接后不断开的socket
当连接成功之后：
    客户端（浏览器）会自动向服务端发送消息，包含： Sec-WebSocket-Key: iyRe1KMHi4S4QXzcoboMmw==
    服务端接收之后，会对于该数据进行加密：base64(sha1(swk + magic_string))
    构造响应头：
            HTTP/1.1 101 Switching Protocols\r\n
            Upgrade:websocket\r\n
            Connection: Upgrade\r\n
            Sec-WebSocket-Accept: 加密后的值\r\n
            WebSocket-Location: ws://127.0.0.1:8002\r\n\r\n        
    发给客户端（浏览器）
建立：双工通道，接下来就可以进行收发数据
    发送数据是加密，解密，根据payload_len的值进行处理
        payload_len <= 125
        payload_len == 126
        payload_len == 127
    获取内容：
        mask_key
        数据
        根据mask_key和数据进行位运算，就可以把值解析出来。

什么是魔法字符串：
客户端向服务端发送消息时，会有一个'sec-websocket-key'和'magic string'的随机字符串(魔法字符串)
#服务端接收到消息后会把他们连接成一个新的key串，进行编码、加密，确保信息的安全性

## GIL

```
GIL是python中的全局解释器锁，是不可控的，同一个进程中，假如有多个线程在运行，那么其中一个线程在运行的时候就会霸占GIL锁，就使得其他线程无法运行，等该线程运行结束以后，其他线程才能运行。如果线程中遇到耗时操作(I/O密集型任务)，则解释器锁会解开，使得其他线程运行，所以说在多线程中，线程的运行仍是有先后顺序的，并不是同时进行。
```

## 互斥锁

```
互斥锁（Mutual Exclusion Lock，简称mutex）是一种同步原语，用于在多线程环境中保护共享资源，确保同一时间只有一个线程可以访问该资源。互斥锁的主要目的是防止多个线程同时修改共享数据，从而避免数据竞争和不一致性问题。
```


## WSGI (Web Server Gateway Interface):

```
⦁ 这是一个Python Web应用程序或框架与Web服务器之间的一种接口协议。
⦁ 它是由Python社区制定的一个规范,用于规定Web服务器如何与Python应用程序进行通信。
⦁ WSGI不是服务器、Python模块、框架、API或任何软件,它只是一种规范。
```

## uWSGI:

```
⦁ 这是一个全功能的HTTP服务器,实现了WSGI协议、uwsgi协议、http协议等。
⦁ 它是一个快速的、自我修复的、开发人员友好的应用容器服务器。
⦁ uWSGI旨在为部署WSGI应用提供一个完整的解决方案。
```

## uwsgi:

```
 这是uWSGI服务器自有的协议,用于定义传输信息的类型(type of information)。
uwsgi协议是一种线路协议而不是通信协议,用于描述uWSGI服务器与其他网络服务器的通信。
```


## ORM

```
ORM是"对象关系映射"(Object-Relational Mapping)的缩写，是一种程序设计技术，用于实现面向对象编程语言里不同类型系统的数据之间的转换。ORM主要解决了对象与关系型数据库之间的数据转换问题。

ORM的主要特点和优势：

1. 对象映射：将数据库表映射到编程语言中的类。

2. 简化数据库操作：开发者可以使用面向对象的方式来操作数据库，而不需要直接编写SQL语句。

3. 数据库无关性：ORM通常支持多种数据库，可以较容易地切换不同的数据库系统。

4. 提高开发效率：减少了手动编写SQL和处理结果集的工作。

5. 安全性：大多数ORM工具会自动处理SQL注入等安全问题。

6. 缓存机制：许多ORM框架提供查询缓存功能，可以提高应用性能。

常见的ORM框架和工具：

1. Java: Hibernate, MyBatis
2. Python: SQLAlchemy, Django ORM
3. Ruby: Active Record (Rails)
4. .NET: Entity Framework
5. PHP: Doctrine, Eloquent (Laravel)

使用ORM的潜在缺点：

1. 学习成本：需要学习ORM框架的特定API和用法。
2. 性能开销：在某些复杂查询场景下，ORM生成的SQL可能不如手写的优化。
3. 抽象泄漏：有时可能需要了解底层SQL才能解决某些问题。
```

## 同源策略

同源策略（Same-Origin Policy）指来自不同源（协议、域名、端口三者中任意一个不同）的请求

## 算法时间复杂度

- 找出基本操作
- 计算基本操作执行的次数
- 去掉低阶项，常数项

基本操作：常数 用加法
顺序结构：常数 用加法
循环结构：乘法 
分支结构：取最大值


### 举例说明：

```python
def bubble(arr):
    n = len(arr)	 # 基本操作 1次
	for i in range(n): # 循环结构 用乘法  n 
		for j in range(n-i-1):  # 循环结构 用乘法 n
			if arr[j] < arr[j+1]: # 分支结构 取最大值 ：
				arr[j],arr[j+1] = arr[j+1], arr[j] # 基本操作1次
	return arr
```

最坏的情况下，内层循环会执行n-1，n-2,n-3 .... 1次
计算过程 ： n  * （n-1+1)*(n-1)/2 * 3   在这里的3属于常数项，去掉，剩下结果为n*n

### 常见算法复杂度：

#### 常数复杂度：O(1)

```python
def f(item):
	return item[0]
```

#### 线性时间复杂度O(n)

```python
def f(arr, target):
	for i in range(len(arr)): # 循环结构 n
		if arr[i] == target:  # 分支结构 最坏的情况下，每次循环都会执行这个判断 
			return i
```

计算过程：n * 2  , 去掉常数项，O(n)

#### 二次时间复杂度

[[001_Python基础#举例说明：]]

#### 对数时间复杂度

```python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1

```

这里主体部分是while 循环，即循环结构，其它的都是常数项，那么循环执行多少次呢？
第一次执行时后，需要查找的数组长度为n/2, 依次为：
- n
- n/2
- n/ 2 * 2
- n/ 2 * 2 * 2
- n/ 2 * (k-1)

假如第k次找到了目标，那么 n/ 2 * (k-1) = 1, k = 1 + log2n, 去掉常数项，则复杂度为 O(log2n)

#### 线性对数时间

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr

    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])

    return merge(left, right)

```## 算法时间复杂度

- 找出基本操作
- 计算基本操作执行的次数
- 去掉低阶项，常数项

基本操作：常数 用加法
顺序结构：常数 用加法
循环结构：乘法 
分支结构：取最大值


### 举例说明：

```python
def bubble(arr):
    n = len(arr)	 # 基本操作 1次
	for i in range(n): # 循环结构 用乘法  n 
		for j in range(n-i-1):  # 循环结构 用乘法 n
			if arr[j] < arr[j+1]: # 分支结构 取最大值 ：
				arr[j],arr[j+1] = arr[j+1], arr[j] # 基本操作1次
	return arr
```

最坏的情况下，内层循环会执行n-1，n-2,n-3 .... 1次
计算过程 ： n  * （n-1+1)*(n-1)/2 * 3   在这里的3属于常数项，去掉，剩下结果为n*n

### 常见算法复杂度：

#### 常数复杂度：O(1)

```python
def f(item):
	return item[0]
```

#### 线性时间复杂度O(n)

```python
def f(arr, target):
	for i in range(len(arr)): # 循环结构 n
		if arr[i] == target:  # 分支结构 最坏的情况下，每次循环都会执行这个判断 
			return i
```

计算过程：n * 2  , 去掉常数项，O(n)

#### 二次时间复杂度

[[001_Python基础#举例说明：]]

#### 对数时间复杂度

```python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1

```

这里主体部分是while 循环，即循环结构，其它的都是常数项，那么循环执行多少次呢？
第一次执行时后，需要查找的数组长度为n/2, 依次为：
- n
- n/2
- n/ 2 * 2
- n/ 2 * 2 * 2
- n/ 2 * (k-1)

假如第k次找到了目标，那么 n/ 2 * (k-1) = 1, k = 1 + log2n, 去掉常数项，则复杂度为 O(log2n)

#### 线性对数时间

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr

    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])

    return merge(left, right)

```


#### 互斥锁(同步锁)

多个线程抢占资源时会造成数据混乱的问题，可以通过加锁来解决，看代码：

```python
import os,time

from threading import Thread,Lock

def work():
    global n
    lock.acquire() #加锁
    temp=n
    time.sleep(0.1)
    n=temp-1
    lock.release()
    
    with lock:
        temp=n
        time.sleep(0.1)
        n=temp-1
if __name__ == '__main__':
    lock=Lock()
    n=100
    l=[]
    for i in range(100):
        t=Thread(target=work)
        l.append(t)
        t.start()
    for t in l:
        t.join()

    print(n) #结果肯定为0，由原来的并发执行变成串行，牺牲了执行效率保证了数据安全
```
加锁之后，数据不会出现混乱的问题了，这种情况称之为线程安全。

锁的单例模式

创建锁的时候，我们还可以采用单例模式，看下面的示例：

```python
from threading import Thread,Lock

class SingleTon:
    __instance = None
    lock = Lock()

    def __new__(cls, *args, **kwargs):
        if cls.__instance:
            return cls.__instance
        with cls.lock:
            if not cls.__instance:
                cls.__instance = super().__new__(cls)
            return cls.__instance

def fun():
    s = SingleTon()
    print(id(s))

for i in range(20):
    t1 = Thread(target=fun,)
    t1.start()
```

#### 死锁

​	进程也有死锁与递归锁，进程的死锁和线程的是一样的，而且一般情况下进程之间是数据不共享的，不需要加锁，由于线程是对全局的数据共享的，所以对于全局的数据进行操作的时候，要加锁。

​	所谓死锁： 是指两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程，如下就是死锁：

**现象1：将自己锁死**
```python

import time
from threading import Lock

mutexA=Lock()
mutexA.acquire()
mutexA.acquire()
print(123)
mutexA.release()
mutexA.release()
```

**现象2：锁嵌套引起的死锁**

``` python
import time
from threading import Thread,Lock

mutexA=Lock()
mutexB=Lock()

class MyThread(Thread):
    def run(self):
        self.func1()
        self.func2()
        
    def func1(self):
        mutexA.acquire()
        print('\033[41m%s 拿到A锁>>>\033[0m' %self.name)
        mutexB.acquire()
        print('\033[42m%s 拿到B锁>>>\033[0m' %self.name)
        mutexB.release()
        mutexA.release()

    def func2(self):
        mutexB.acquire()  
        print('\033[43m%s 拿到B锁???\033[0m' %self.name)
        time.sleep(2)
        #分析：当线程1执行完func1，然后执行到这里的时候，拿到了B锁，线程2执行func1的时候拿到了A锁，那么线程2还要继续执行func1里面的代码，再去拿B锁的时候，发现B锁被人拿了，那么就一直等着别人把B锁释放，那么就一直等着，等到线程1的sleep时间用完之后，线程1继续执行func2，需要拿A锁了，但是A锁被线程2拿着呢，还没有释放，因为他在等着B锁被释放，那么这俩人就尴尬了，你拿着我的老A，我拿着你的B，这就尴尬了，俩人就停在了原地

        mutexA.acquire()
        print('\033[44m%s 拿到A锁???\033[0m' %self.name)
        mutexA.release()

        mutexB.release()

if __name__ == '__main__':
    for i in range(10):
        t=MyThread()
        t.start()
```

#### 递归锁

死锁的解决方法：递归锁，在Python中为了支持在同一线程中多次请求同一资源，python提供了可重入锁RLock。
这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源。上面的例子如果使用RLock代替Lock，则不会发生死锁：

**现象1的解决**

```python
import time
from threading import RLock as Lock

mutexA=Lock()
mutexA.acquire()
mutexA.acquire()
print(123)
mutexA.release()
mutexA.release()
```

**现象2的解决**

```python
import time
from threading import Thread,RLock
fork_lock = noodle_lock = RLock()

def eat1(name):
    noodle_lock.acquire()
    print('%s 抢到了面条'%name)
    fork_lock.acquire()
    print('%s 抢到了叉子'%name)
    print('%s 吃面'%name)
    fork_lock.release()
    noodle_lock.release()

def eat2(name):
    fork_lock.acquire()
    print('%s 抢到了叉子' % name)
    time.sleep(1) 
    noodle_lock.acquire()
    print('%s 抢到了面条' % name)
    print('%s 吃面' % name)
    noodle_lock.release()
    fork_lock.release()

for name in ['taibai','wulaoban']:
    t1 = Thread(target=eat1,args=(name,))
    t1.start()
for name in ['alex','peiqi']:
    t2 = Thread(target=eat2,args=(name,))
    t2.start()
```

#### GIL锁和锁的区别

```
GIL VS Lock

  机智的同学可能会问到这个问题，就是既然你之前说过了，Python已经有一个GIL来保证同一时间只能有一个线程来执行了，为什么这里还需要lock? 

  首先我们需要达成共识：锁的目的是为了保护共享的数据，同一时间只能有一个线程来修改共享的数据

  然后，我们可以得出结论：保护不同的数据就应该加不同的锁。

  最后，问题就很明朗了，GIL 与Lock是两把锁，保护的数据不一样，前者是解释器级别的（当然保护的就是解释器级别的数据，比如垃圾回收的数据），后者是保护用户自己开发的应用程序的数据，很明显GIL不负责这件事，只能用户自定义加锁处理，即Lock

  过程分析：所有线程抢的是GIL锁，或者说所有线程抢的是执行权限

  线程1抢到GIL锁，拿到执行权限，开始执行，然后加了一把Lock，还没有执行完毕，即线程1还未释放Lock，有可能线程2抢到GIL锁，开始执行，执行过程中发现Lock还没有被线程1释放，于是线程2进入阻塞，被夺走执行权限，有可能线程1拿到GIL，然后正常执行到释放Lock。。。这就导致了串行运行的效果

　　既然是串行，那我们执行

　　t1.start()

　　t1.join

　　t2.start()

　　t2.join()

  这也是串行执行啊，为何还要加Lock呢，需知join是等待t1所有的代码执行完，相当于锁住了t1的所有代码，而Lock只是锁住一部分操作共享数据的代码。
```

## 锁，条件变量，信号量

```from chatgpt
1.  锁（Lock）：一种基本的同步原语，用于在多个线程之间提供独占访问。只有获取锁的线程可以修改共享资源，其他线程必须等待锁被释放后才能获取锁。Python 中常用的锁包括 `RLock` 和 `Semaphore`。
2.  条件变量（Condition）：一种高级同步原语，用于在线程之间共享复杂状态的情况下进行同步。条件变量提供了一个线程等待的机制，可以等待某个状态变为满足条件时才继续执行。Python 中的条件变量通过 `threading.Condition` 类实现。
3.  信号量（Semaphore）：一种计数器，用于在多个线程之间控制并发访问的数量。当一个线程需要访问某个共享资源时，它需要先获取一个信号量，如果没有可用的信号量，则线程会被阻塞。Python 中的信号量通过 `threading.Semaphore` 类实现。

锁、条件变量和信号量都是线程同步的工具，但是它们的使用场景不同，需要根据具体的情况进行选择。例如，当多个线程需要互斥访问某个共享资源时，可以使用锁来实现；当线程需要等待某个事件或条件时，可以使用条件变量；当需要控制并发访问数量时，可以使用信号量。
```

Ref: 

[Threading Semaphore in Python](https://superfastpython.com/thread-semaphore/)
## 事务

数据库的事务是一组作为单一逻辑工作单元执行的操作序列。事务具有四个关键特性，通常称为ACID属性：
原子性（Atomicity）确保事务中的所有操作要么全部完成，要么全部不完成；
一致性（Consistency）保证数据库在事务执行前后都保持一致的状态；
隔离性（Isolation）使得并发执行的事务之间不会互相影响；
持久性（Durability）确保一旦事务提交，其结果就是永久性的。
事务的这些特性使得数据库能够在复杂的操作中维护数据的完整性和一致性，即使在出现系统故障的情况下也能保证数据的可靠性。

From claude-3-5-sonnet@20240620, input:28, output: 232

## 内存管理机制
 python中垃圾回收机制主要有三方面:引用记数为主,标记清除,分代回收为辅

### 引用计数(没有人记得你时,才是真正的死亡)

在python中一切皆为对象,每个对象都维护一个引用次数,如果次数为零,即没有任何引用,它将被回收机制无情的收割(没有人赢得你时,才是真正的死亡.鲁迅也曾说:有的人死了,但他仍活着,我想也有此意思).下面看看具体代码:

```python
import sys


class Person:
    pass


p = Person() # p被创建,指向Person对象,记数 +1

print("p ref count:", sys.getrefcount(p)) # p作为实参传给函数,记数 +1,总次数为 2
p1 = p  # p1引用 ,记数 +1 总次数为 3
print("p ref count", sys.getrefcount(p)) 
del p1 # 删除 p1对p的引用,次数-1, 总次数为2
print("p ref count", sys.getrefcount(p)) # 2

#输出:
p ref count: 2
p ref count 3
p ref count 2
```

#### 第一次打印为什么是2?
我们来看源码
```python
def getrefcount(): # real signature unknown; restored from __doc__
    """
    Return the reference count of object.
    
    The count returned is generally one higher than you might expect,
    because it includes the (temporary) reference as an argument to
    getrefcount().
    """
    pass
```

可以清楚的看到,结果比我们预想的要高一个,是因为变量本身作为getrefcount的临时引用,所以会+1,所以结果为2

#### 函数为什么会引用+2?
我们先看看下面这种情况:
- 创建 +1
- getrefcount +1
那也只有2才对,可是结果为什么是4呢?

```python
import sys


class Person:
    pass

def log_ref(var):
    print(sys.getrefcount(var))

p = Person()
# 输出:
4
```

在对象传给函数时,函数内部有两个属性`func_globals, __globals__`都会引用该参数,所以此时该对象的引用计数会 +2. 需要注意的是在python3中我们通过dir无法查看到 `func_globals`.
在python2中,函数包含两个属性:`func_globals, __globals__`,在python3中前者的命名发生了改变,具体可以参考: https://docs.python.org/3.1/whatsnew/3.0.html
Operators And Special Methods:
> The function attributes named func_X have been renamed to use the __X__ form, freeing up these names in the function attribute namespace for user-defined attributes. To wit, func_closure, func_code, func_defaults, func_dict, func_doc, func_globals, func_name were renamed to __closure__, __code__, __defaults__, __dict__, __doc__, __globals__, __name__, respectively.


#### 既然对象作为参数传递给函数引用会+2,那么下面这段代码为结果为什么是2?
很明显 getrefcount也是函数,那打印结果应该是3才对,这是因为getrefcount会自动处理这种情况
```python
import sys


class Person:
    pass


p = Person()
print("p ref count:", sys.getrefcount(p))
```

#### 作为容器的元素的场景
这里的容器以列表为例:

```python
import sys


class Person:
    pass


p = Person()

l = [p, ]

print(sys.getrefcount(p))
# 输出3
```

#### +1场景总结:
引用记数+1场景:
- 对象被创建
- 对象被引用
- 对象作为参数,传入函数中
- 对象作为对象存储在容器中

#### 对象被显式销毁
主动将你忘记.但此时你已经不存在,无法通过`getrefcount()`来测试引用数量

#### 对象的引用被指新的对象
没错,你被绿了.你对象移情别恋了,她心里只有另一个人了,记得你的人就少了一个

```python
import sys


class Person:
    pass


you = Person()
your_gf = you
print(sys.getrefcount(you))  # 你对象还爱你的时候
another_handsome_boy = Person()
your_gf = another_handsome_boy
print(sys.getrefcount(you))  # 你对象爱上高富帅的时候
#输出:
3
2
```

#### 离开作用域
在getrefcount函数中,记数会+1,那如果这样,我不停打印不会就不断增加吗?但离开了getrefcount的世界,它就把你忘记了
```python
import sys


class Person:
    pass


you = Person()
print(sys.getrefcount(you))
print(sys.getrefcount(you))
#输出
2
2
```
#### 销毁容器
当你的世界被销毁时:
```python
import sys


class Person:
    pass


p = Person()

l = [p, ]
del l
print(sys.getrefcount(p))
# 输出2
```

#### -1场景总结
- 显式销毁
- 引用被指向新的对象
- 离开作用域
- 容器被销毁

### 标记清除
引用记数无法解决的问题:
```python
import sys


class Person:
    pass


you = Person()
your_gf = Person()
you.gf = your_gf
your_gf.bf = you
print(sys.getrefcount(you))
print(sys.getrefcount(your_gf))
# 输出
3
3
```
除去你getrefcount引用,你和你对象相亲相爱,所以每人有两个引用.此时即出现了循环引用.
我们看官方文档: 只有容器类型,会存在这种循环引用,而对于简单原子数据类型如 数字,字符串不支持垃圾回收,或者不存储对其它对象引用的容器也不支持.

> Python’s support for detecting and collecting garbage which involves circular references requires support from object types which are “containers” for other objects which may also be containers. Types which do not store references to other objects, or which only store references to atomic types (such as numbers or strings), do not need to provide any explicit support for garbage collection.

ref: https://docs.python.org/3.1/c-api/gcsupport.html?highlight=circular%20reference

如果我们显式删除,会导致无法查看getrefcount, 我们借助第三方库来查看对对象的引用数,注意count的参数为字符串:
```python
pip install objgraph

import sys
import objgraph


class Person:
    pass


you = Person()
your_gf = Person()
print(objgraph.count("Person"))


del your_gf
print(objgraph.count("Person"))
# 输出
2
1
```

#### 循环引用
彼此相爱的两人,任谁也分不开

```python
import sys
import objgraph


class Person:
    pass


you = Person()
your_gf = Person()
print(objgraph.count("Person"))

you.gf = your_gf
your_gf.bf = you

del you
del your_gf
print(objgraph.count("Person"))

# 输出
2
2
```

借助objgraph 打印出这种节点图:

```python
import sys
import objgraph


class Boy:
    pass


class Girl:
    pass


you = Boy()
your_gf = Girl()
print(objgraph.count("Boy"))
print(objgraph.count("Girl"))
you.gf = your_gf
your_gf.bf = you
# del you
# del your_gf
print(objgraph.count("Boy"))
print(objgraph.count("Girl"))
objgraph.show_backrefs([you,your_gf])
```
上面的代码中虽然在最后的print语句时仍能打印,但如果我们删除了`you, your_gf`则也无法打印出图形,会报错
我们通过引用图:

![](https://github.com/Andy963/notePic/blob/main/circular_ref.png?raw=true)
![](https://github.com/Andy963/notePic/blob/main/circular_ref.png)

#### python的解决办法
python会收集所有的容器对象,放在一个双向链表中,将一个对象和它引用的对象的引用数都-1,如果它们的引用数变成0,则说明它们之间存在循环引用,那么这两个对象将标记出来,并被无情清除. 如果你和你的对象私定终生,他们总有办法发现的,尤其是他们不同意的时候.

### 分代回收
Python解释器在垃圾回收时，会遍历链表中的每个对象，如果存在循环引用，就将存在循环引用的对象的引用计数器 -1，同时Python解释器也会将计数器等于0（可回收）和不等于0（不可回收）的一分为二，把计数器等于0的所有对象进行回收，把计数器不为0的对象放到另外一个双向链表表（即：分代回收的下一代）
分代回收的代，有三代，按年轻到老的顺序为：0代，1代，2代 
门限，有三个门限 ，门限0，门限1，门限2，默认情况下为700，10，10 

```python
import gc

print(gc.get_threshold())
# (700, 10, 10)
```
第一个参数表示:垃圾回收器中新增的对象个数-消亡的对象个数,当这个值达到700以上时,会触发检测机制.
简单点讲:当新生儿出生数,减去死亡人数大于700,将导致这种检测(这时候就该计划生育了),然后开始检测,当0代的检测10次后(你只生了一胎,不信,检测10次,确定你只生了一胎),才会检测1代(此时0代的检测到第11次了),然后1代检测10次后才会检测2代(此时0代已经检测到101次).

> The GC classifies objects into three generations depending on how many collection sweeps they have survived. New objects are placed in the youngest generation (generation 0). If an object survives a collection it is moved into the next older generation. Since generation 2 is the oldest generation, objects in that generation remain there after a collection. In order to decide when to run, the collector keeps track of the number object allocations and deallocations since the last collection. When the number of allocations minus the number of deallocations exceeds threshold0, collection starts. Initially only generation 0 is examined. If generation 0 has been examined more than threshold1 times since generation 1 has been examined, then generation 1 is examined as well. Similarly, threshold2 controls the number of collections of generation 1 before collecting generation 2. 

如果要修改这些门限: 调用`set_threshold()`即可

#### 启用回收
垃圾回收机制默认是开启的:
```python
import gc

print(gc.isenabled()) # True
gc.disable()
```

#### 手动回收
我们以前面的循环引用为例:
```python
import gc
import objgraph


class Boy:
    pass


class Girl:
    pass


you = Boy()
your_gf = Girl()
print(objgraph.count("Boy"))
print(objgraph.count("Girl"))
you.gf = your_gf
your_gf.bf = you
del you
del your_gf
gc.collect()
print(objgraph.count("Boy"))
print(objgraph.count("Girl"))
```
在未手动触发垃圾回收时,两次都输出
```
1
1
1
1
```
启用后则是:
```python
1
1
0
0
```

