
拉格朗日中值定理

泰勒展开

### 洛必达法则

> 洛必达法则（L'Hôpital's rule）是一种用于计算极限的方法。它用于解决形如“0/0”或“∞/∞”这样的不定式极限问题。
> 
> 当我们在计算一个分式的极限时，如果分子和分母都趋近于 0 或无穷大，那么这个极限就是一个不定式极限。在这种情况下，我们可以使用洛必达法则来计算这个极限。
> 
> 洛必达法则的基本思想是：对于一个形如“0/0”或“∞/∞”的不定式极限，我们可以将分子和分母同时求导，然后再计算新分式的极限。这个新分式的极限就是原分式的极限。
> 
> 例如，假设我们要计算以下分式的极限：
> 
> $$
> \lim_{x \to 0} \frac{\sin x}{x}
> $$
> 
> 当 $x$ 趋近于 0 时，分子 $\sin x$ 和分母 $x$ 都趋近于 0，因此这是一个“0/0”的不定式极限。根据洛必达法则，我们可以将分子和分母同时求导，得到：
> 
> $$
> \lim_{x \to 0} \frac{\sin x}{x} = \lim_{x \to 0} \frac{\cos x}{1} = \cos 0 = 1
> $$
> 
> 因此，原分式的极限为 1。


### 计算图模型

命令式编程

声明式编程


### 损失函数

#### 平方差损失

#### 交叉熵损失

### 凸函数

> 凸函数是指定义在某个向量空间的凸子集上的实值函数。对于凸子集中任意两个向量x1和x2，都有f((x1+x2)/2) ≤ (f(x1)+f(x2))/2成立。³
> 
> 直观地理解，凸函数的图像形如开口向上的杯，而相反，凹函数则形如开口向下的帽。¹

 凸函数的二阶导大于零

马尔科夫假设

### 向量

标量

#### 加法运算:
两个张量对应的元素相加。例如:

```
A = [[1, 2], 
     [3, 4]]
B = [[5, 6],
     [7, 8]]

A + B = [[1+5, 2+6], 
         [3+7, 4+8]]
     = [[6, 8], 
        [10, 12]]
```

#### 缩放运算:
张量每个元素与一个系数相乘。例如:

```
A = [[1, 2], 
     [3, 4]]

2 * A = [[1*2, 2*2], 
         [3*2, 4*2]] 
     = [[2, 4], 
        [6, 8]]
```

#### 矩阵乘法:
当两个张量的维度匹配时,通过对应元素乘积和求和得到一个新张量。例如:

```
A = [[1, 2],      B = [[5, 6],
     [3, 4]]           [7, 8]]   

A * B = [[1*5+2*7, 1*6+2*8],  
         [3*5+4*7, 3*6+4*8]]
     = [[19, 22],  
        [43, 50]]
```

#### 转置:
交换张量的两个索引对应的元素。例如:

```
A = [[1, 2],   A的转置 = [[1, 3],  
     [3, 4]]        [2, 4]]
```

#### 迹:
对角线元素之和。例如:

```
A = [[1, 2],   A的迹 = 1 + 4 = 5  
     [3, 4]]
```
### 监督学习
监督学习是指在训练阶段使用带标签的数据集，模型通过学习输入和输出之间的关系来进行预测。

### 无监督学习
无监督学习是指在训练阶段使用不带标签的数据集，模型通过分析输入数据的结构和模式来进行学习。


## 分类问题

垃圾邮件检测，图像分类（人脸识别）

> 逻辑回归
用于解决分类问题的一种模型。根据数据特征或属性,计算其归属于某一类别
的概率P(x),根据概率数值判断其所属类别。主要应用场景:二分类问题。

sigmoid函数

$$p(x) =\frac{1}{1+e^{-x}}$$

$$p(x) = \frac{1}{1+e^{g(x)}}$$
其中的g(x) 被称为decision boundary 决策边界

$$g(x)=\theta+\theta_1x_1+\theta_2x_2+...$$



### 线性回归



均方误差（越小越好），R方（越接近1越好）

### 逻辑回归实现二分类

```python
from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression()
lr_model.fit(x,y)

# 边界函数系统
theta1, theta2 = lr_model.coef_[0][0],lr_model.coef_[0][1]
theta0 = lr_model.intercept_[0]

# 对新数据做预测
predictions = lr_model.predict(x_new) # x_new 为输入参数？？

#计算准确率

from sklearn.metrics import accuracy_score

y_predict = lr_model.predict(x)
accuracy = accuracy_score(y, y_predict)

# 画图看决策边界效果，可视化
plt.plot(x1, x2_boundary)
passed = plt.scatter(x1[mask], x2[mask])
failed = plt.scatter(x[~mask], x2[~mask], marker='^')
```

### chip

```python
#load the data  
import pandas as pd  
import numpy as np  
data = pd.read_csv('chip_test.csv')  
# data.head()

# 提取 test1 和 test2 列 这遭殃为需要的数据库
X = data[['test1', 'test2']]  
y = data['pass']  
# 计算新列  
X1_2 = X['test1'] ** 2  
X2_2 = X['test2'] ** 2  
X1_X2 = X['test1'] * X['test2']  
  
# 创建新的 DataFrame  
X_new = pd.DataFrame({'X1': X['test1'], 'X2': X['test2'], 'X1_2': X1_2, 'X2_2': X2_2, 'X1_X2': X1_X2})  
  
# 打印新的 DataFrame  
print(X_new)


#establish the model and train it  
from sklearn.linear_model import LogisticRegression  
LR2 = LogisticRegression()  
LR2.fit(X_new,y)

# 查看准确率
from sklearn.metrics import accuracy_score  
y2_predict = LR2.predict(X_new)  
accuracy2 = accuracy_score(y,y2_predict)  
print(accuracy2)


```


so how to use the trained model to predict new data:

```python
from sklearn.linear_model import LogisticRegression

# 假设您已经拟合了一个逻辑回归模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 准备新数据
new_data = [[0.3, 0.7], [0.1, 0.9]]

# 使用模型预测新数据的类别
predictions = model.predict(new_data)

# 打印预测结果
print(predictions)
```

save the model and realod it next time

```python
from sklearn.externals import joblib

# 假设您已经拟合了一个模型
model = LogisticRegression()
model.fit(X, y)

# 将模型导出并保存到磁盘中
joblib.dump(model, 'model.joblib')

# 从磁盘中加载模型
model = joblib.load('model.joblib')

```

## 聚类问题

### K-means
均值聚类  无监督

```python
from sklearn.cluster import KMeans
KM = KMeans(n_clusters=3, random_state=0)
KM.fit(x)

-`n_clusters`：这个参数指定了要在数据中形成的簇的数量。在上面的示例中，`n_clusters=3` 意味着算法将尝试将数据分成 3 个簇。
-`random_state`：这个参数控制了随机数生成器的种子。它可以用于确保算法的结果是可重复的。在上面的示例中，`random_state=0` 意味着每次运行算法时都会使用相同的随机数种子，从而产生相同的结果
- 请注意，KMeans 算法是一种迭代算法，它从一组初始簇中心开始，并不断更新簇分配和簇中心，直到满足收敛条件。由于初始簇中心的选择是随机的，因此算法的结果可能会因随机数种子的不同而有所不同。通过设置 `random_state` 参数，您可以确保每次运行算法时都使用相同的随机数种子，从而获得可重复的结果。
```

get the centers of the model
```python
centers = KM.cluster_centers_
```

准确率计算 "Accuracy calculation"

```python 
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y, y_predict)
```

预测结果矫正 "Prediction result correction"

```python
y_cal = [ ]
for i in y_predict:
	if i == 0:
		y_cal.append(2)
	elif i == 1:
		y_cal.append(1)
	else:
		y_cal.append(0)
# 针对标签对不上的情况
```

以空间中k个点为中心，对最靠近它们的点归类

### KNN
近邻分类  有监督（已标记好的数据,必须告诉它分几类）

model train

```python
from sklearn.neighbors import KNeighborsClassifier
KNN = KNeighborsClassifier(n_neighbors=3)
KNN.fit(x,y)
```

### meanShift
均值漂移， 基于密度梯度上升方向聚类  使用向量

自动计算带宽（半径）"Automatic bandwidth (radius) calculation"
```python
from sklearn.cluster import MeanShift, estimate_bandwidth

bandwidth = estimate_bandwidth(x, n_samples=500)
```

建立模型并训练  build model and train

```python
ms = MeanShift(bandwidth=bandwidth)
ms.fit(x)
```


### MLP

max-pooling : 最大法池化
avg-pooling: 平均池化


## 神经网络


构建模型示例

```python
# None表示数量不确定3072为图片的维数
x = tf.placeholder(tf.float32, [None, 3072])
# 离散变量,且为一维
y = tf.placeholder(tf.int64,[None])

# 输入通过x.get_shape()[-1], 输出是1维直接给定1
# 初始化
with tf.variable_scope('scope_name', reuse=tf.AUTO_REUSE):
    w =tf.get_variable('w',[x.get_shape()[-1],1], initializer=tf.random_normal_initializer(0,1))
# 偏置,初始化为0
    b=tf.get_variable('b',[1], initializer=tf.constant_initializer(0.0))

y_ = tf.matmul(x, w)+b

# y = 1时的概率 [None,1],所以要将y reshape成[None,1]
p_y_1 = tf.nn.sigmoid(y_)
#[None,1]
y_reshaped = tf.reshape(y, (-1, 1))
# 变换成float32类型
y_reshaped_float = tf.cast(y_reshaped, tf.float32)
# 计算损失函数, reduce_mean 平均值 square 是平方
loss = tf.reduce_mean(tf.square(y_reshaped_float - p_y_1))
#bool
predict = p_y_1 > 0.5
correct_prediction = tf.equal(tf.cast(predict, tf.int64), y_reshaped)
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float64))

predict=p_y_1>0.5
#[1,0,1,1,1,0,0,0]
correct_prediction = tf.equal(tf.cast(predict, tf.int64),y_reshaped)
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float64))

# 梯度下降的方法
# 1e-3 初始rate
with tf.name_scope('train_op'):
    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)
```

### 卷积神经网络

卷积神经网络结构：
全卷积神经网络 = 卷积层 + 池化层

#### 卷积

#### padding

#### 池化

池化的作用：
- 常使用不重叠、不补零
- 没有用于求导的参数
- 池化层参数为步长和池化核大小
- 用于减少图像尺寸，从而减少计算量
- 一定程度解决平移鲁棒
- 损失了空间位置精度 

#### 全连接层

将上一层输出展开并连接到每一个神经元上

#### 神经网络训练优化

随机梯度下降：每次只使用一个样本
Mini-Batch梯度下降：每次使用小部分数据进行训练

梯度下降存在局部极值和saddle point的问题， 由此提出动量梯度下降

动量梯度下降的特点：
开始训练时，积累动量，加速训练
局部极值附近震荡时，梯度为0，由于动量，跳出陷阱
梯度改变方向的时候，动量缓解动荡

卷积--- 解决问题
- 局部连接
- 参数共享